{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA WRANGLING WITH MONGODB\n",
    "## OpenStreet Map Project\n",
    "##  The Greater Seattle Area\n",
    "The goal of this project is to assess the quality of data for validity, accuracy, completeness, and uniformity. I will be choosing the Greater Seattle Area map for this data wrangling project as I live there.\n",
    "\n",
    "The Greater Seattle area is the hub of the many world's top companies like Amazon, Boeing, Microsoft, and Starbucks Coffee. It is a rapidly growing region of the United States with the rise of tech-related jobs attracting talents from across the globe. With such growth comes many data about the area's many exciting places.   \n",
    "\n",
    "Using Open-Street Map dataset, a data wrangling task will be performed to analyze whether the integrity of the data stored there is intact; that is, if there are missing values or the data is accurate.\n",
    "\n",
    "\n",
    "# Problems with Dataset\n",
    "# Street Names Issues\n",
    "One of the problems I came across with the map dataset is that some street names are abbreviated and therefore corrections are needed to format them into full street names. For example, *'S'* must be changed to *'South'* and *'Ave'* to *'Avenue'*. \n",
    "\n",
    "Implementing a regular expression to match the street names is the first step in fixing issues with the abbreviated street names.\n",
    "\n",
    "The regular expression below is a way to process the necessary changes.\n",
    "\n",
    "***\n",
    "`\n",
    "street_type_re = re.compile(r'\\b\\S+\\.?$', re.IGNORECASE)`\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the majority of the abbreviated street names  were successfully changed to full street names, a problem arose with some street names that begin with ,for example, 'S' as in 'South' not correctly formated. The examples below show the existing problems.\n",
    "\n",
    "* *72nd Ave S => 72nd Ave South*\n",
    "* *Wells Ave S => Wells Ave South*\n",
    "* *Williams Ave S => Williams Ave South*\n",
    "* *Burnett Ave S => Burnett Ave South*\n",
    "* *Park Ave N => Park Ave North*\n",
    "* *S 2nd St => S 2nd Street*\n",
    "* *S 212th St => S 212th Street*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To overcome the obstacle above, update_name function was used. Here's how to correct the problem the abbreviations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_name(name, mapping):\n",
    "    \n",
    "    key = mapping.keys()\n",
    "    \n",
    "    best_name =\"\"\n",
    "    for nm in name.split(\" \"):\n",
    "        \n",
    "        if nm in key:\n",
    "            \n",
    "            bn = mapping[nm]\n",
    "            if best_name==\"\":\n",
    "                best_name +=bn\n",
    "            else:\n",
    "                best_name = best_name + \" \" + bn\n",
    "        else:\n",
    "            if best_name==\"\":\n",
    "                best_name +=nm\n",
    "            else:\n",
    "                best_name = best_name + \" \" + nm\n",
    "         \n",
    "    return best_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result of running the above function is shown below:\n",
    "* *72nd Ave S => 72nd Avenue South*\n",
    "* *Wells Ave S => Wells Avenue South*\n",
    "* *Williams Ave S => Williams Avenue South*\n",
    "* *Burnett Ave S => Burnett Avenue South*\n",
    "* *Park Ave N => Park Avenue North*\n",
    "* *S 2nd St => South 2nd Street*\n",
    "* *S 212th St => South 212th Street*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview of the Data\n",
    "After going through the arduous process of wrangling the dataset, Mongo dB is used to statistically analyzed the dataset. Here all the interesting questions will be answered using the Mongo dB query methods.\n",
    "\n",
    "For example to know the size of the file without using the the dabatase query methods generated the sizes below after converting the xml file to a json format.\n",
    "* *seattle.osm 169.6 MB*\n",
    "* *seattle.osm.json 184.9 MB*\n",
    "\n",
    "### Size of the file\n",
    "To retrieve the size of the selected area of the map, the Mongo dB *count()* method is applied.\n",
    "*collection.find().count()* returns the number of documents in the database collection which is the same the size of the file.\n",
    "\n",
    "`The file size is : 834061` \n",
    "\n",
    "### The number of unique users\n",
    "Using this query method: \n",
    "***\n",
    "`len(collection.distinct('created.user'))`\n",
    "*** \n",
    "resulted in the number of unique users available in the dataset which is  \n",
    "\n",
    "***\n",
    "**`925`**\n",
    "***\n",
    "\n",
    "### The number of nodes and ways\n",
    "To find the number of nodes, apply this method\n",
    "***\n",
    "`collection.find({'type':'way'}).count()`\n",
    "***\n",
    "\n",
    "`Nodes: 111,469`\n",
    "\n",
    "### The Top 5 Banks\n",
    "Knowing the number of banks in the area helps existing and new customers to choose the closest and most frequent banks to avoid a long commute.\n",
    "\n",
    "***\n",
    "`top_5_banks = list(collection.aggregate([{'$match': {'amenity': 'bank'}}, \n",
    "                                {'$group': {'_id': '$name', \n",
    "                                            'count': {'$sum': 1}}}, \n",
    "                                {'$sort': {'count': -1}}, \n",
    "                                {'$limit': 5}]))\n",
    " print(top_5_banks)`\n",
    "***                            \n",
    "                                \n",
    "`[{'_id': 'Chase', 'count': 7},\n",
    " {'_id': 'Bank of America', 'count': 6},\n",
    " {'_id': 'Wells Fargo', 'count': 6},\n",
    " {'_id': 'KeyBank', 'count': 5},\n",
    " {'_id': 'U.S. Bank', 'count': 5}]`\n",
    " \n",
    "### Most Popular Cafe\n",
    "Since this region is the home and hub of the world's renowned technological and other companies, it is interesting to find how many Starbucks are there and its closest competitors.\n",
    "\n",
    "***\n",
    "`\n",
    "top_cafes = list(collection.aggregate([{'$match': {'amenity': 'cafe'}}, \n",
    "                                {'$group': {'_id': '$name', \n",
    "                                            'count': {'$sum': 1}}}, \n",
    "                                {'$sort': {'count': -1}}, \n",
    "                                {'$limit': 5}]))\n",
    "print(top_cafes) `\n",
    "***\n",
    "***\n",
    "`[{'_id': 'Starbucks', 'count': 33},\n",
    " {'_id': None, 'count': 8},\n",
    " {'_id': 'BigFoot Java', 'count': 4},\n",
    " {'_id': 'LadyBug Bikini Espresso', 'count': 3},\n",
    " {'_id': 'Mighty Mugs Coffee', 'count': 2}]`\n",
    " ***\n",
    " \n",
    "### Top 5 Restaurants\n",
    "\n",
    "The dominant restaurants in the area are listed accordingly. It is surprising that Pizza Hut has more locations than the rest of the restaurants.\n",
    "***\n",
    "`\n",
    "restaurants = list(collection.aggregate([{'$match': {'amenity': 'restaurant'}}, \\\n",
    "                                {'$group': {'_id': '$name', \\\n",
    "                                            'count': {'$sum': 1}}}, \\\n",
    "                                {'$sort': {'count': -1}}, \\\n",
    "                                {'$limit': 5}]))`\n",
    "***\n",
    "***\n",
    "`\n",
    "[{'_id': 'Pizza Hut', 'count': 6},\n",
    " {'_id': \"Denny's\", 'count': 4},\n",
    " {'_id': 'IHOP', 'count': 3},\n",
    " {'_id': \"Applebee's\", 'count': 2},\n",
    " {'_id': 'Tortas Locas', 'count': 2}]`\n",
    "***\n",
    "\n",
    "### Top 5 Fast-Food Places\n",
    "\n",
    "***\n",
    "`\n",
    "list(collection.aggregate([{'$match': {'amenity': 'fast_food'}}, \\\n",
    "                                {'$group': {'_id': '$name', \\\n",
    "                                            'count': {'$sum': 1}}}, \\\n",
    "                                {'$sort': {'count': -1}}, \\\n",
    "                                {'$limit': 5}]))`\n",
    "\n",
    "***\n",
    "`\n",
    "[{'_id': 'Subway', 'count': 21},\n",
    " {'_id': \"McDonald's\", 'count': 12},\n",
    " {'_id': 'Jack in the Box', 'count': 10},\n",
    " {'_id': 'Taco Time', 'count': 6},\n",
    " {'_id': \"Wendy's\", 'count': 6}]\n",
    "`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other Ideas About The Dataset\n",
    "\n",
    "Majority of the fields in the map dataset are correctly formatted but there is more room for improvement when it comes to some fields that are missing values. For example, upon further wrangling the dataset, when it comes to some businesses and organizations, the names of the cities they located in are missing. The address portion of the data is mostly blank.\n",
    "\n",
    "Here is the sample of the dataset after the query was conducted:\n",
    "\n",
    "\n",
    "\n",
    "***\n",
    "`\n",
    "collection.aggregate([{'$match': {'phone':{ '$regex':'^253'}}},        \n",
    "                       {'$group': {'_id': '$name','phone':{'$push':'$phone'}, 'address':{'$push':'$address.city'}, \\\n",
    "                                   'count': {'$sum': 1}}}, \\\n",
    "                       {'$sort': {'count': -1}}, {'$limit':10}])`\n",
    "\n",
    "\n",
    "***\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "***\n",
    "\n",
    "```\n",
    "[{'_id': 'Afford-A-Vet Animal Clinic',\n",
    "  'phone': ['253-859-8387'],\n",
    "  'address': [],\n",
    "  'count': 1},\n",
    "  \n",
    " {'_id': 'WinCo Foods', \n",
    "  'phone': ['253-850-8818'], \n",
    "  'address': [],\n",
    "  'count': 1},\n",
    "  \n",
    " {'_id': 'Grace Fellowship of Kent',\n",
    "  'phone': ['253-854-4248'],\n",
    "  'address': [],\n",
    "  'count': 1},\n",
    " \n",
    " {'_id': 'New Hope Presbyterian Church',\n",
    "  'phone': ['253-859-8998'],\n",
    "  'address': [],\n",
    "  'count': 1},\n",
    "```\n",
    "\n",
    "***\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "After wrangling the Greater Seattle area map dataset, the contributors did mostly a great job in minimizing errors with the dataset. I believe with the technological advancement, data-entry errors will be detected in no time and corrected accordingly by the future auto-correcting data-entry forms through  well-built supervised learning algorithms where data are cross referenced prior to official submissions.\n",
    "\n",
    "**Benefits** \n",
    "\n",
    "One of the advantages of having auto-correcting data-entry forms is that the contributors do not waste their time fixing the problems themselves. Another benefit is that it removes barrier to entry for those who want to be part of an open-source community and therefore volunteer their time to make the map dataset well-documented, valid, and error-free.\n",
    "\n",
    "**Anticipated problems**\n",
    "\n",
    "The over-arching challenges facing any open-source platforms are the volume of the data submitted. In the case of the OSM dataset, there are likely issues with duplicate entries that cannot be easily detected in time until a verification process is in place. Duplicated entries cause  previous data to be overwritten and therefore contribute to missing values. For example, as documented in the database query section, some businesses and organizations are missing the cities they operate in. Also another problem that faces the OSM dataset is the integrity of the data and whether the volunteers are qualified to do the contributions.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
